{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ddpm_unet_cattn import SPVUnet\n",
    "import torch\n",
    "import lightning as L\n",
    "from models.g_spvd import GSPVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "# steps_to_run = [32, 16, 8, 4, 2]\n",
    "steps_to_run = [250]\n",
    "# steps_to_run = [1000, 500, 250, 125, 63, 32, 16, 8, 4, 2]\n",
    "on_all = True\n",
    "distilled = False\n",
    "scheduler = 'ddim'\n",
    "\n",
    "categories = ['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.shapenet.shapenet_loader import ShapeNet\n",
    "\n",
    "path = \"../data/ShapeNet\"\n",
    "\n",
    "test_dataset = ShapeNet(path, \"val\", 2048, categories, load_renders=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hyperparams import load_hyperparams\n",
    "\n",
    "hparams_path = f'../checkpoints/distillation/GSPVD/{\"-\".join(categories)}/hparams.yaml'\n",
    "\n",
    "hparams = load_hyperparams(hparams_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'voxel_size' : hparams['voxel_size'],\n",
    "    'nfs' : hparams['nfs'], \n",
    "    'attn_chans' : hparams['attn_chans'], \n",
    "    'attn_start' : hparams['attn_start'], \n",
    "    'cross_attn_chans' : hparams['cross_attn_chans'], \n",
    "    'cross_attn_start' : hparams['cross_attn_start'], \n",
    "    'cross_attn_cond_dim' : hparams['cross_attn_cond_dim'],\n",
    "}\n",
    "\n",
    "model = SPVUnet(**model_args)\n",
    "model = GSPVD(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_schedulers.ddpm_scheduler import DDPMSparseScheduler\n",
    "from my_schedulers.ddim_scheduler import DDIMSparseScheduler\n",
    "from utils.helper_functions import process_ckpt\n",
    "\n",
    "def get_sched(steps, dist, scheduler):\n",
    "    if scheduler == 'ddim':\n",
    "        sched = DDIMSparseScheduler(\n",
    "            beta_min=hparams['beta_min'], \n",
    "            beta_max=hparams['beta_max'], \n",
    "            steps=steps, \n",
    "            init_steps=hparams['n_steps'],\n",
    "            mode=hparams['mode'],\n",
    "        )\n",
    "    elif dist:\n",
    "        sched = DDPMSparseScheduler(\n",
    "            beta_min=hparams['beta_min'], \n",
    "            beta_max=hparams['beta_max'], \n",
    "            steps=steps, \n",
    "            init_steps=hparams['n_steps'],\n",
    "            mode=hparams['mode'],\n",
    "        )\n",
    "    else:\n",
    "        sched = DDPMSparseScheduler(\n",
    "            beta_min=hparams['beta_min'], \n",
    "            beta_max=hparams['beta_max'], \n",
    "            steps=steps, \n",
    "            init_steps=steps,\n",
    "            mode=hparams['mode'],\n",
    "        )\n",
    "    return sched\n",
    "\n",
    "def get_ckpt(steps, dist, scheduler):\n",
    "    if dist:\n",
    "        ckpt_path = f'../checkpoints/distillation/GSPVD/{\"-\".join(categories)}/{steps}-steps.ckpt'\n",
    "        # ckpt_path = f'../checkpoints/distillation/GSPVD/{\"-\".join(categories)}/new/{steps}-steps.ckpt'\n",
    "    elif scheduler == 'ddim':\n",
    "        ckpt_path = f'../checkpoints/distillation/GSPVD/{\"-\".join(categories)}/1000-steps.ckpt'\n",
    "    else:\n",
    "        ckpt_path = f'../checkpoints/distillation/GSPVD/{\"-\".join(categories)}/{steps}-steps.ckpt'\n",
    "\n",
    "    # ckpt_path = '/home/ubuntu/SPVD_Lightning/checkpoints/distillation/GSPVD/car/intemediate/250-steps/250-steps-epoch=399.ckpt'\n",
    "    ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "    ckpt = process_ckpt(ckpt)\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from metrics.chamfer_dist import ChamferDistanceL2\n",
    "from metrics.PyTorchEMD.emd import earth_mover_distance as EMD\n",
    "from utils.helper_functions import normalize_to_unit_sphere, standardize, normalize_to_unit_cube\n",
    "from schedulers.factory import create_sparse_scheduler\n",
    "\n",
    "from metrics.rgb2point import chamfer_distance, EMDLoss\n",
    "\n",
    "emd_loss = EMDLoss()\n",
    "\n",
    "def run_test(steps):\n",
    "    CD = ChamferDistanceL2()\n",
    "    \n",
    "    sched = get_sched(steps, distilled, scheduler)\n",
    "    # sched = create_sparse_scheduler() # Chair, Car\n",
    "\n",
    "    ckpt = get_ckpt(steps, distilled, scheduler)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    cd_mean = 0\n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for datapoint in tqdm(test_loader):\n",
    "            ref_pc = datapoint['pc'].cuda()\n",
    "            features = datapoint['render-features'].cuda()\n",
    "\n",
    "            B, N, C = ref_pc.shape\n",
    "            gen_pc = sched.sample(model, B, N, reference=features)\n",
    "            \n",
    "            ref_pc = ref_pc - ref_pc.mean(dim=1, keepdim=True)\n",
    "            # Point Clouds should have the max distance from the origin equal to 0.64\n",
    "            r = (ref_pc * ref_pc).sum(dim=-1).sqrt().max(dim=1, keepdim=True)[0]\n",
    "            #print(f'Max radius: {r.shape}')\n",
    "            #print(ref_pc.shape)\n",
    "            ref_pc = ref_pc / r.unsqueeze(-1) * 0.64\n",
    "            # Shuffle Points in each point cloud of the batch\n",
    "            ref_pc = ref_pc[:, torch.randperm(ref_pc.shape[1])]\n",
    "            ref_pc = ref_pc[:, :1024] # Take only 1024 points from each point cloud\n",
    "\n",
    "            gen_pc = gen_pc - gen_pc.mean(dim=1, keepdim=True)\n",
    "            # Point Clouds should have the max distance from the origin equal to 0.64\n",
    "            r = (gen_pc * gen_pc).sum(dim=-1).sqrt().max(dim=1, keepdim=True)[0]\n",
    "            # print(f'Max radius: {r}')\n",
    "            gen_pc = gen_pc / r.unsqueeze(-1) * 0.64\n",
    "            # Shuffle Points in each point cloud of the batch\n",
    "            gen_pc = gen_pc[:, torch.randperm(gen_pc.shape[1])]\n",
    "            gen_pc = gen_pc[:, :1024]\n",
    "\n",
    "            for g, r in tqdm(zip(ref_pc, gen_pc), leave=False):\n",
    "                g = g.detach().cpu()\n",
    "                r = r.detach().cpu()\n",
    "                cd_mean += chamfer_distance(g, r, direction='bi') / 2\n",
    "            \n",
    "            # emd_mean += emd_loss(ref_pc, gen_pc)\n",
    "            \n",
    "            n += B\n",
    "            # print(f\"CD: {cd_mean / n}\")\n",
    "        \n",
    "    cd_mean /= n\n",
    "       \n",
    "    print(f\"Steps: {steps}, CD: {cd_mean}\")\n",
    "    \n",
    "    return cd_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_means(cds):\n",
    "    path = f'../metrics/{\"-\".join(categories)}'\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    filename = f\"{path}/{'distilled' if distilled else 'skip'}.res\"\n",
    "    string = \"\"\n",
    "    for steps in sorted(cds.keys(), reverse=True):\n",
    "        cd = np.array(cds[steps])\n",
    "        string += f\"{steps}, {cd.mean()}, {cd.std()}, {cd.min()}\\n\"\n",
    "        \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(string)\n",
    "        \n",
    "    print(f\"Saved means to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cds = defaultdict(list)\n",
    "for steps in steps_to_run:\n",
    "    try:\n",
    "        cds[steps].append([run_test(steps) for _ in range(3)])\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "else:\n",
    "    save_means(cds)\n",
    "    ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_means(cds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
